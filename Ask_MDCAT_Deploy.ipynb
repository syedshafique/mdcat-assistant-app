{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dFW43YxX4JrJ"
      },
      "outputs": [],
      "source": [
        "# prompt: i want to deploy this on streamlit\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "from docx import Document\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
        "import torch\n",
        "import os # Import os for checking file existence\n",
        "\n",
        "# Define the device for model computation\n",
        "dev = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device for generation: {dev}\")\n",
        "\n",
        "# --- Load Embedding Model ---\n",
        "@st.cache_resource # Cache the embedding model\n",
        "def load_embedding_model():\n",
        "    return SentenceTransformer('all-MiniLM-L6-v2')\n",
        "\n",
        "embedding_model = load_embedding_model()\n",
        "\n",
        "# --- Load Model for Generation ---\n",
        "@st.cache_resource # Cache the generative model and tokenizer\n",
        "def load_generative_model(model_name=\"ByteDance-Seed/Seed-Coder-8B-Reasoning\", device=\"cpu\"):\n",
        "    try:\n",
        "        # Configure quantization (adjust based on your hardware and needs)\n",
        "        # Using 8-bit loading for potential memory savings on GPU\n",
        "        quantization_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True if device == 'cuda' else False,\n",
        "            # load_in_4bit=False,\n",
        "            # bnb_4bit_compute_dtype=torch.float16,\n",
        "            # bnb_4bit_quant_type=\"nf4\",\n",
        "            # bnb_4bit_use_double_quant=False,\n",
        "        )\n",
        "\n",
        "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "        model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config)\n",
        "\n",
        "        # Move model to device if not using 8-bit or 4-bit loading\n",
        "        if not (quantization_config.load_in_8bit or getattr(quantization_config, 'load_in_4bit', False)):\n",
        "            model.to(device)\n",
        "\n",
        "        return tokenizer, model\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading the generative model: {e}\")\n",
        "        st.warning(\"Please ensure the model name is correct and necessary libraries (like bitsandbytes) are installed if using 8-bit loading.\")\n",
        "        return None, None\n",
        "\n",
        "tokenizer, model = load_generative_model(device=dev)\n",
        "\n",
        "# --- Data Loading Functions ---\n",
        "# Ensure these functions can load from paths accessible by the Streamlit app\n",
        "# For Streamlit Cloud, you might need to upload these files or access them differently.\n",
        "# For local deployment, ensure the paths are correct relative to where you run the script.\n",
        "\n",
        "def load_sts_data(csv_path):\n",
        "    if not os.path.exists(csv_path):\n",
        "        st.error(f\"STS CSV file not found at {csv_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        return pd.read_csv(csv_path, encoding='latin-1')\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading STS data from {csv_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "def load_past_paper(docx_path):\n",
        "    if not os.path.exists(docx_path):\n",
        "        st.error(f\"Past Papers DOCX file not found at {docx_path}\")\n",
        "        return None\n",
        "    try:\n",
        "        doc = Document(docx_path)\n",
        "        qa_pairs = []\n",
        "        # Assuming the questions and answers are in the first table\n",
        "        if not doc.tables:\n",
        "             st.warning(f\"No tables found in {docx_path}. Cannot load QA pairs.\")\n",
        "             return []\n",
        "\n",
        "        table = doc.tables[0]\n",
        "        rows = [cell.text.strip() for row in table.rows for cell in row.cells if cell.text.strip()]\n",
        "        for i in range(0, len(rows), 2):\n",
        "            question = rows[i]\n",
        "            answer = rows[i+1] if i+1 < len(rows) else ''\n",
        "            qa_pairs.append({'question': question, 'answer': answer})\n",
        "        return qa_pairs\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error loading Past Papers data from {docx_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# --- Generate Response Using the loaded LLM ---\n",
        "def generate_llm_response(query, retrieved_answer, dev):\n",
        "    if model is None or tokenizer is None:\n",
        "        return \"Generative model not loaded properly.\"\n",
        "\n",
        "    prompt = f\"You are an intelligent MDCAT assistant. Use the following info to answer the user's question.\\n\\nQuestion: {query}\\n\\nRetrieved Info: {retrieved_answer}\\n\\nAnswer:\"\n",
        "    try:\n",
        "        # Move input tensors to the specified device\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\").to(dev)\n",
        "\n",
        "        # Generate response using the generative model\n",
        "        # Use `input_ids` and `attention_mask` from inputs\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs.input_ids,\n",
        "            attention_mask=inputs.attention_mask,\n",
        "            max_new_tokens=256,\n",
        "            do_sample=True,\n",
        "            top_k=50,\n",
        "            temperature=0.7\n",
        "        )\n",
        "        # Decode the generated tokens, skipping the input prompt tokens\n",
        "        response = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "        return response.strip()\n",
        "    except Exception as e:\n",
        "        st.error(f\"Error during LLM response generation: {e}\")\n",
        "        return \"An error occurred during response generation.\"\n",
        "\n",
        "# --- RAG Pipeline with LLM Identification ---\n",
        "def ask_mdcat_assistant_smart(query, sts_data, past_papers, embedding_model, dev, similarity_threshold=0.7):\n",
        "    \"\"\"\n",
        "    Answers a query by first checking STS and Past Papers datasets\n",
        "    using semantic search. If a confident match is found in either,\n",
        "    it retrieves the relevant answer and uses the LLM to generate a response.\n",
        "    If no confident match is found in either, the LLM answers on its own.\n",
        "    \"\"\"\n",
        "    sts_match = None\n",
        "    past_papers_match = None\n",
        "    sts_best_score = -1\n",
        "    past_papers_best_score = -1\n",
        "\n",
        "    # Prepare STS data for semantic search\n",
        "    sts_qa_pairs = []\n",
        "    if sts_data is not None:\n",
        "        sts_qa_pairs = [{'question': q, 'answer': a} for q, a in zip(sts_data['query'].tolist(), sts_data['answer'])]\n",
        "\n",
        "    # Semantic search in STS data\n",
        "    if sts_qa_pairs:\n",
        "        try:\n",
        "            sts_questions = [item['question'] for item in sts_qa_pairs]\n",
        "            sts_question_embeddings = embedding_model.encode(sts_questions, convert_to_tensor=True)\n",
        "            query_embedding = embedding_model.encode(query, convert_to_tensor=True)\n",
        "            sts_cos_scores = util.cos_sim(query_embedding, sts_question_embeddings)\n",
        "            sts_best_idx = sts_cos_scores.argmax()\n",
        "            sts_best_score = sts_cos_scores[0][sts_best_idx].item()\n",
        "\n",
        "            if sts_best_score >= similarity_threshold:\n",
        "                sts_match = sts_qa_pairs[sts_best_idx]\n",
        "                st.info(f\"Confident match found in STS data with score: {sts_best_score:.4f}\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error during STS semantic search: {e}\")\n",
        "\n",
        "\n",
        "    # Semantic search in Past Papers data\n",
        "    if past_papers:\n",
        "        try:\n",
        "            past_papers_questions = [item['question'] for item in past_papers]\n",
        "            past_papers_question_embeddings = embedding_model.encode(past_papers_questions, convert_to_tensor=True)\n",
        "            query_embedding = embedding_model.encode(query, convert_to_tensor=True) # Re-encode query\n",
        "            past_papers_cos_scores = util.cos_sim(query_embedding, past_papers_question_embeddings)\n",
        "            past_papers_best_idx = past_papers_cos_scores.argmax()\n",
        "            past_papers_best_score = past_papers_cos_scores[0][past_papers_best_idx].item()\n",
        "\n",
        "            if past_papers_best_score >= similarity_threshold:\n",
        "                past_papers_match = past_papers[past_papers_best_idx]\n",
        "                st.info(f\"Confident match found in Past Papers data with score: {past_papers_best_score:.4f}\")\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error during Past Papers semantic search: {e}\")\n",
        "\n",
        "\n",
        "    retrieved_info = None\n",
        "    source = \"LLM (General Knowledge)\" # Default source if no confident match\n",
        "\n",
        "    # Determine which source to use based on confidence score\n",
        "    if sts_match and past_papers_match:\n",
        "        if sts_best_score > past_papers_best_score:\n",
        "            retrieved_info = sts_match['answer']\n",
        "            source = \"STS Data\"\n",
        "        else:\n",
        "            retrieved_info = past_papers_match['answer']\n",
        "            source = \"Past Papers Data\"\n",
        "    elif sts_match:\n",
        "        retrieved_info = sts_match['answer']\n",
        "        source = \"STS Data\"\n",
        "    elif past_papers_match:\n",
        "        retrieved_info = past_papers_match['answer']\n",
        "        source = \"Past Papers Data\"\n",
        "\n",
        "    st.write(f\"Using source: {source}\")\n",
        "\n",
        "    if retrieved_info:\n",
        "        st.write(\"Generating LLM response using retrieved info...\")\n",
        "        response = generate_llm_response(query, retrieved_info, dev)\n",
        "    else:\n",
        "        st.write(\"No confident match found in datasets. LLM answering directly...\")\n",
        "        if model is None or tokenizer is None:\n",
        "             return \"Generative model not loaded properly.\"\n",
        "\n",
        "        # Construct a prompt for the LLM to answer on its own\n",
        "        direct_prompt = f\"You are an intelligent MDCAT assistant. Answer the following question:\\n\\nQuestion: {query}\\n\\nAnswer:\"\n",
        "        try:\n",
        "            inputs = tokenizer(direct_prompt, return_tensors=\"pt\").to(dev)\n",
        "\n",
        "            outputs = model.generate(\n",
        "                input_ids=inputs.input_ids,\n",
        "                attention_mask=inputs.attention_mask,\n",
        "                max_new_tokens=256,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                temperature=0.7\n",
        "            )\n",
        "            response = tokenizer.decode(outputs[0][inputs.input_ids.shape[-1]:], skip_special_tokens=True)\n",
        "            response = response.strip()\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error during direct LLM generation: {e}\")\n",
        "            response = \"An error occurred during direct response generation.\"\n",
        "\n",
        "\n",
        "    return response\n",
        "\n",
        "# --- Streamlit UI ---\n",
        "st.title(\"MDCAT Assistant\")\n",
        "st.write(\"Ask me anything about the MDCAT!\")\n",
        "\n",
        "# --- Data File Paths ---\n",
        "# IMPORTANT: Update these paths to where your files are located relative to your Streamlit app.\n",
        "# If deploying to Streamlit Cloud, you'll need to upload these files or use a different storage method.\n",
        "sts_csv_path = 'mdcat_queries.csv' # Example: Assuming the file is in the same directory\n",
        "past_papers_docx_path = 'past_tests.docx' # Example: Assuming the file is in the same directory\n",
        "\n",
        "# Load data\n",
        "sts_data = load_sts_data(sts_csv_path)\n",
        "past_papers = load_past_paper(past_papers_docx_path)\n",
        "\n",
        "# Check if data and models loaded successfully\n",
        "if sts_data is None or past_papers is None or model is None or tokenizer is None:\n",
        "    st.error(\"Required data or models failed to load. The assistant cannot function.\")\n",
        "else:\n",
        "    user_query = st.text_input(\"Enter your question:\")\n",
        "\n",
        "    if user_query:\n",
        "        with st.spinner(\"Thinking...\"):\n",
        "            response = ask_mdcat_assistant_smart(user_query, sts_data, past_papers, embedding_model, dev)\n",
        "        st.subheader(\"Assistant Response:\")\n",
        "        st.write(response)\n",
        "\n",
        "st.markdown(\"---\")\n",
        "st.write(\"Note: This assistant uses a RAG pipeline and a large language model to answer questions.\")\n",
        "\n"
      ]
    }
  ]
}